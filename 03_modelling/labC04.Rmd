---
title: "Lab C4 - Time Series Regression"
output:
  html_document:
    toc: yes
---
  
```{r}
library(tidyverse)
```

## Introduction: Time Series - Basic Concepts

In this workshop, we will introduce a new concept in statistics - **time series**. Time series are observations or measurements that are indexed according to time **regularly**. Examples of time series are the number of newly confirmed COVID-19 cases **per day** in NZ, the global sales of iPhone **per month**, the **quarterly** unemployment rate in NZ, and the **daily** closing value of the NZX50.

*Give a few examples on time series.*

It is easy to notice that all the time series data are collected with a natural temporal ordering. In an abstract sense, we record a time series like $y_1,y_2,...,y_T$, or just $y_t,t=1,...,T$ where $t=1,...,T$ is the **time index**. 

Why does that make things different?

What happened today may affect tomorrow's world but nothing could change the outcomes of yesterday. The time series has a natural temporal ordering which implies a **unidirectional** causal relationship. The structure of time series implied by this ordering distinguishes it from other types of data that are commonly analysed. 

In addition to the time index, it is important to notice that data points in a time series are collected **regularly** over time. In the beginning of this workshop, we highlight the words, 'per day', 'per month', 'quarterly', and 'daily'. These words define the **frequency** or **periodicity** of time series **within one year**, i.e. the number of observations within a fixed time period. The frequency is 12 for monthly time series and 365 for daily time series within one year.

*List a few more adjectives/adverbs describing the frequency.*

*What is the frequency of the wind speed in Wellington per hour within one day?*

*What is the frequency of the daily precipitation in Auckland within one week?*

The above concepts will be pretty easy to understand by exploring some real time series. So we will perform an exploratory data analysis on NZ housing data collected by the Reserve Bank of New Zealand(RBNZ) in Exercise 1. Interestingly, time can still be modelled by our linear model easily and the fitted linear model can be further used to make time series predictions. In Exercise 2 and 3, we will learn how to analyse time series via linear models.

You first need to install `RBNZ` and `janitor` using the `Packages` menu in the bottom right. Just click the install button and type in `RBNZ`, then install. Or you can install the package with R command `install.packages()`. RStudio may have prompted you about installing them when you loaded this file.

## Exercise 1: Exploring NZ House Price

Before we start the formal statistical analysis on any real time series, it is essential to load the data first and then turn the data to a suitable format ready for any further analysis. 

1. The following R code chunk retrieves the quarterly time series on housing from the website of the Reserve Bank of New Zealand (RBNZ) (https://www.rbnz.govt.nz/statistics).

```{r}
library(RBNZ)
library(janitor)
housing.M10 <- getSeries('M10')
housing.M10$meta
housing <- housing.M10$data %>% tibble() %>% clean_names()  
housing
```

RBNZ regularly publishes statistical tables for a range of economic and financial data over time which becomes a rich resource of real time series data. In their website, by clicking `Economic indicators` you will find `M10 Housing` with a downloadable XLSX file on the housing data collected by RBNZ. `M10` is a unique identifier for `Housing` data. Other economic and financial data also have their own unique identifiers which can be found in the same website, for example, `M2 Consumption`, `M5 Gross Domestic Product`, and `M12 Population and migration`.

`allAvailableSeries()` will show you a list of all the unique identifiers time series and you can easily find the link to the supporting information on each time series by inputting `?M10` or `?M2` in RStudio. 

With this identifier `M10`, we use `getSeries()` function from the R package `RBNZ` to load `Housing` data for us automatically as follows. Notice that `getSeries()` returns a list containing `data` (what we need) and `meta` (some meta information on data). So we add `$data` to get the data set. For more information on this data set, we . In addition, we use `clean_names()` from the R package `janitor` to get a clean tibble of `housing`.

`housing` data contains one column for time information (`date`) and four columns for different housing information, i.e. total value of housing stock `m`, residential investment `real_m`, house price index (HPI) `index`, and the number of house sales `number`.

*Specify the frequency of `housing` data.*

2. Let's make a scatter-line plot of house price index over time. You may customise it a bit to get a better appearance. 

```{r}
housing %>% ggplot(aes(x=date,y=index)) + geom_point() + geom_line() +
  xlab('Time') + ylab('House Price Index') + ggtitle('Will Kiwi afford the house price?')
```

The warning message `Removed 1 rows containing missing values (geom_point).` in your HTML reminds you that there is an `NA` value in our data. Let's remove this NA value from the tibble via `drop_na()` as
```{r}
housing <- housing %>% drop_na()
```

We can see clearly over the last three decades HPI was increasing in most years. There are a few exceptions, i.e., around 1998, 2008, 2011. *What happened in these years?* Another feature of this time series is that HPI seems increase with different speeds over different decades. In 1990s, HPI tends to be flatten or just increase moderately. But the first two decades in 21 century look quite crazy. The affordability of NZ house has become a core social issue. Generally speaking, HPI shows an increasing **trend** over the past thirty years. 

What will happen in the next decade with the shock of COVID-19? Will the house price in NZ become more affordable in the future? To answer these questions, it is important for us to extract the long term **trend** from this time series and make predictions on the future HPI. 

3. *Make scatter-line plot for the rest three variables in `housing`. Briefly summarise their features.*

## Exercise 2: Capturing the Trend of House Price

The time index $t=1,2,3,...,T$ implied by the natural temporal ordering is always increasing. A very simple idea is to consider the pairs $(1,y_1)$, $(2,y_2)$,...,$(T,y_T)$ just like the pairs of observation $(x_i,y_i),i=1,2,...,n$ in the linear model. 

If the underlying trend of a time series shows either increasing or decreasing pattern consistently, it is natural to link it with the time index of each observation. *Why?*

Moreover, if the underlying trend looks linear, a simple linear model like `y~t` becomes a perfect candidate to discover the underlying trend in a time series.

Anyway, let's try it on HPI! Before fitting our linear model, it is necessary to expand our tibble by adding the time indices $1,2,3,...$ as follows.

```{r}
housing.ts <- housing %>% mutate(time=1:n())
housing.ts
```

*Why we don't use `date` immediately?*

1. Now we can fit a simple linear regression model as follows
```{r}
hpi.lm <- lm(index~time,data=housing.ts)
summary(hpi.lm)
```

*Comment on the R summary of this model.*

We can further visualise the fitted linear model as 
```{r}
library(visreg)
visreg(hpi.lm, gg=TRUE) + xlab('Time Index') + ylab('House Price Index')
```

*Comment on the goodness of fit of the linear trend model based on the visualisation.*

*Perform a standard residual diagnostics for your fitted linear model by following the steps in Lab C3.*

**Optional Challenge: Make a log transformation on HPI and fit another linear model. Compare the R summary with the model without transformation. Perform the standard residual diagnostics on this model.** 

2. A group of talented school students studied HPI before they learned the log transformation. They noticed that HPI was increasing slowly at the beginning but increasing faster later. In other words, the increase in HPI occurs at an increasing rate for each successive time period. The students suggested that we may use a **quadratic** function instead of the linear function to model the trend as follows.
\[
mean(y_t)=a+bt+ct^2.
\]
But the students didn't know how to find the coefficients $a,b,c$. Can we help?

The answer is YES! Interestingly, this can be done via `lm()` easily as follows.

```{r}
hpi.qm <- lm(index~time+I(time^2), data=housing.ts)
summary(hpi.qm)
```

**It is important to use `I(time^2)`, not `time^2` if you want to fit a quadratic trend to a time series.**

We can find an additional row in `Coefficients:` as `I(time^2)`. There is the estimate for $c$ as you may expected. The interpretation of rest is just like what we have done in Lab C2. One interesting thing is that the estimate for $b$ becomes insignificant! 

*Compare the R summary with that in Step 1. Discuss your findings.*

Visualising this model can be done with `visreg()` in the usual way, but let's use the `broom` library instead to do it here:

```{r}
library(broom)
augment(hpi.qm) %>% ggplot(aes(x = time ,y=index)) + geom_point() + geom_line() +
        geom_line(aes(y=.fitted),col='red') + xlab('Time Index') + ylab('House Price Index')
```

You may add the confidence band or prediction band by following Exercise 1 in Lab C3. 

Sometimes people prefer making a plot in the calendar time rather than in the time index. This can be done easily by using `broom` as follows. Notice that we add the original data set `housing.ts` into `augment()` and change `x = time` to `x = date` in `ggplot()`. This is very handy in many cases especially you are fitting a linear model with transformation. 

```{r}
library(broom)
augment(hpi.qm,housing.ts) %>% ggplot(aes(x = date ,y=index)) + geom_point() + geom_line() +
        geom_line(aes(y=.fitted),col='red') + xlab('Time') + ylab('House Price Index')
```

*Comment on the goodness of fit of the quadratic trend model based on the visualisation.*

3. Now we have two candidate models from Step 1 and 2 for HPI time series. 
A natural question arises: Which model is better? 

Both R summary and visualisation can be used. The residual analysis is another tool to find the better model. While these tools are useful, we have a more delicate way to compare the above two models performance. Particularly, the linear model can be regarded as a special case of the quadratic model. If we set $c=0$, the quadratic model reduces to the linear model. If two models can be linked in such manner, we call these two models are **nested**. The simple model is usually called a **reduced model** (linear one in this case) and the complicated model is usually called an **extended model** (quadratic one in this case).

An **ANOVA (ANalysis Of VAriance) test** can be used to compare two nested candidate models as
```{r}
anova(hpi.lm, hpi.qm)
```

`anova()` tests if the additional parameter(s) $c$ in the quadratic model is zero or not, i.e. the null hypothesis is $c=0$ and the p-value is reported by `Pr(>F)`. *What does the output of `anova()` suggest?* If the P-value is small we can conclude that that `Model 2` - the quadratic trend model is better. 

It is easy to find the improvement in goodness of fit from the quadratic model is significant for HPI time series. In fact, with the additional coefficient $c$, the quadratic trend model (and any extended model) is guaranteed to get closer to the real time series than the linear trend model(and any reduced model). 

*A pure quadratic model as $y=a+cx^2$ is also nested with the quadratic model $y=a+bx+cx^2$. Why? Use ANOVA to tell which model is better.*

However, the quadratic trend model is more complicated than the linear trend model and the pure quadratic model as it has one more coefficient. If the improvement is just marginal, we may prefer the linear model or the pure quadratic model as they are so concise. The ANOVA test examines if the extended model (quadratic trend model ) explains sufficient variability in $y$ compared to the reduced model (linear trend model) at a price of adding one more parameter. We will revisit this topic in Lab C6.

4. *Predict HPI for Q3 and Q4 of 2020 with both linear trend model and quadratic trend model with 95% prediction intervals. Comment on your predictions.*

## Exercise 3: Addressing Seasonalities in Consumption

Besides the trend extracted by our linear models, another important feature of time series is the **seasonality** or **seasonal effects**. Many things exhibit seasonal patterns over different time periods (e.g. months or quarters) of a year. For example:
    
  - Sales of ski equipment are very high during winter and almost nonexistent during other seasons.

  - Sales at department stores around Christmas holidays are much higher than during other times of the year.

  - Power bills are expected to be higher during winter than in other seasons. 

*Enumerate a few more time series with a seasonal pattern.*

Clearly, seasonality is closely related to the frequency and their linkages can be expected just by reading the words 'monthly', 'weekly', or 'quarterly'. In fact, in a time series the seasonal pattern for a particular quarter or month is exactly repeated by the frequency of the time series. 

Obviously, it is necessary to include the seasonal effects in time series modelling in most cases. Otherwise, the prediction becomes rather superficial as it only accounts for the long-term trend. 

How can we make use of the seasonal effects? We will start by exploring the data set `M2 Consumption` from RBNZ.

1. First of all, we load `consumption.M2` by `getSeries()` with the unique identifier `M2`. We further process the the data via extracting the first two columns, i.e. the time information `date` and the retail trade sales `x_m`. We also filter the data by keeping only the observations before 2019 with `filter()` and `ymd()`. The rest observation will be reserved for the later usage. 

See `consumption.M2$meta`, `?M2` and/or the RBNZ's official website (https://www.rbnz.govt.nz/statistics/m2) for more information on this time series. 

```{r}
library(lubridate)
consumption.M2 <- getSeries('M2')
consumption <- consumption.M2$data %>% tibble() %>% clean_names() 
consumption
retail.trade.sales <- consumption %>%
  select(date,x_m) %>%
  filter(date < ymd('2019-01-01'))
retail.trade.sales %>%
  ggplot(aes(x=date,y=x_m)) + geom_point() + geom_line() + 
  xlab('Time') + ylab('Retail Trade Sales')
```

*Specify the frequency of this time series.*

We can clearly see an increasing trend over these years in general. The regular fluctuations of sales within one year suggest strong seasonal effects. *Why?* One shall also notice that the seasonal variability is increasing over time.  

2. First of all, we can extract a linear trend from this time series as follows

```{r}
rts.ts <- retail.trade.sales %>% mutate(time=1:n())
rts.lm <- lm(x_m~time,data=rts.ts)
summary(rts.lm)
```

The R output reads pretty good with all coefficient significant and a very high $R^2=0.9457$! 

Let's visualise the model as follows:
```{r}
visreg(rts.lm, gg=TRUE) + xlab('Time Index') + ylab('Retail Trade Sales')
```

Oops. A simple line won't be able to capture the seasonality.

3. Let's perform a standard residual diagnostics on the fitted model.

```{r}
plot(rts.lm)
```

Except for Q-Q plot, rest diagnostic plots look really really weird! *Add a few detailed comments on each plot.*

In addition to the above standard diagnostic plots, the **residuals versus time** plot is frequently used in the residual diagnostics of time series modelling as

```{r}
augment(rts.lm) %>%
  ggplot(aes(x=time,y=.resid)) + geom_point() + geom_line() +
  ggtitle('Residuals versus Time Index') + xlab('Time Index')
```

The pattern in the **residuals versus time** plot seems containing the information on the seasonality! Such pattern actually suggests that the residuals violate the i.i.d. condition, i.e. the independently and identically distributed condition. We can conjecture that the residuals in different seasons is coming from different distributions and they are correlated in the time order. *How can we check the internal correlation in residuals?*

We further make a box plot to compare the residuals over different quarters.
Here we add the data set `rts.ts` to `augment()` to make sure that we have the access to the calendar time and we can get the corresponding quarter by `quarter()` from `lubridate()`. 

```{r}
augment(rts.lm, rts.ts) %>% mutate(quarter=quarter(date)) %>%
  ggplot(aes(y=.resid,x=quarter)) + geom_boxplot(aes(group=quarter))
```


Each box in the above boxplot characterises the residuals, i.e. the deviations from the trend line, at a specific quarter. The residuals here are a combination of true random errors and seasonal effects. 

How can we extract the information on seasonality? A simple idea is to estimate the seasonal effect at a particular quarter by the mean of residuals at this quarter. This will add a constant shift to the trend line for each quarter. The rest deviations will be regarded as the final residuals. 

*Try to compute the residuals mean of each quarter by `summarise().`*

4. The above procedures seems work well but tedious. A surprising fact is that we can complete these jobs with one line by `lm()` as follows. The trick is that we must turn `quarter` into a factor (categorical, qualitative) variable via `factor()` and include it in the regression formula in `lm()`.

```{r}
rts.ts.q <- rts.ts %>% mutate(quarter=factor(quarter(date))) 
rts.lm.q <- lm(x_m~time+quarter,data=rts.ts.q)
summary(rts.lm.q)
```

We have obtained a few more rows in `Coefficients:`, including `quarter2`,`quarter3`,`quarter4` with `Estimate`, `Std. Error`, etc. 

Looks good! Oh, wait, where is `quarter1`?

In fact, the seasonal effect at the first quarter is included in `(Intercept)`$=True.Intercept + Seasonal.Effect.of.Q1$! More importantly, `Estimate`s for the rest three quarters is not the seasonal effects at corresponding quarters. They are instead the difference between $Seasonal.Effect.of.Q1$ and $Seasonal.Effect.of.Q2.Q3.and .Q4$

Therefore, we can write the follow equation for our fitted linear model as
\[
mean(Retail.Trade.Sales)=7240.8 + 162.6 \times Time + D_Q, Q=1,2,3,4.
\]
where $D_1=0$, $D_2=-330.0$,$D_3=-278.0$ and $D_4=1521.8$.

More details on this strange issue after adding factor variables in the linear model will be revealed in the coming Lectures C5 and C6, and corresponding Labs.

Visualise our model with both linear trend and seasonal effects:
```{r}
library(broom)
augment(rts.lm.q) %>% ggplot(aes(x = time ,y=x_m)) + geom_point() + geom_line(alpha=0.3) +
  geom_line(aes(y=.fitted),col='red') + xlab('Time Index') + ylab('Retail Trade Sales')
```

Now the fitted curve looks much more reasonable! *Do you think there is any room for further improvement?*

*Perform a time series residual analysis on your fitted model. Remember the residuals versus time plot.*

*Try to visualise the model with `visreg()`. What do you find from the plots of `visreg()`.*

5. We can predict the sales in 2019 as follows
```{r}
newtime <- data.frame(time=nrow(rts.ts)+(1:4),quarter=factor(1:4))
augment(rts.lm.q,newdata=newtime)
```

*Compute the mean square errors of your prediction in 2019.*

*What will happen if we use the fitted model to predict the sales in 2020?*

**Optional Challenge: Will log transform improve your model fit? Try it!**


